{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62f1b37a2bd4486ca3208649dc188db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1432b72627b243aa8fcc38b238d29026",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bc2c53448819447c91bafc4f7c05a6ca",
              "IPY_MODEL_d965f76a15ad4ce2841dabe13d53ee3a"
            ]
          }
        },
        "1432b72627b243aa8fcc38b238d29026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc2c53448819447c91bafc4f7c05a6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7d5c467c17234b1391ab29f328fa739f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66f1e729d303408586c3ee7162447542"
          }
        },
        "d965f76a15ad4ce2841dabe13d53ee3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dcce2be355b5464596212e32e31fd1cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [00:02&lt;00:00, 4391306.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_915c54a9ceae470e9f9b872813e6aa59"
          }
        },
        "7d5c467c17234b1391ab29f328fa739f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66f1e729d303408586c3ee7162447542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcce2be355b5464596212e32e31fd1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "915c54a9ceae470e9f9b872813e6aa59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a518a5a070e4b11a1ec0e400e36127d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_221169360e024a5d9549bd71d3659c75",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac18fa24fb43413bb57156666c9f4221",
              "IPY_MODEL_cbf408df5b54491b90f11c673880e192"
            ]
          }
        },
        "221169360e024a5d9549bd71d3659c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac18fa24fb43413bb57156666c9f4221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9123ae8f33484115804b7341a88c3970",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ad50b1dc39f4bd9a15974a7ba6eb7ce"
          }
        },
        "cbf408df5b54491b90f11c673880e192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d765e3b8e1704342b16d1790998ab05b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [01:34&lt;00:00, 315.64it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a11f045e61943d9af4c5a00ab732163"
          }
        },
        "9123ae8f33484115804b7341a88c3970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ad50b1dc39f4bd9a15974a7ba6eb7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d765e3b8e1704342b16d1790998ab05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a11f045e61943d9af4c5a00ab732163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "795059627a4542a088d677993caf6e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cdd7b55c1e5945deafcc5158d48368b3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4fdc7b18d55844f7b591ed297cccbff3",
              "IPY_MODEL_009ed755de634f99a0cb84989b3a09af"
            ]
          }
        },
        "cdd7b55c1e5945deafcc5158d48368b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4fdc7b18d55844f7b591ed297cccbff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b545fe508f64d79a8c4bf1f9f54fd97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c338b3a76d1d4fc7a06add09acb83a36"
          }
        },
        "009ed755de634f99a0cb84989b3a09af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67b677369a9648f78ca1e7cf70e48f1d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:20&lt;00:00, 81334.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dd8db42caad4edf92e547314a8cd6ef"
          }
        },
        "5b545fe508f64d79a8c4bf1f9f54fd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c338b3a76d1d4fc7a06add09acb83a36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67b677369a9648f78ca1e7cf70e48f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dd8db42caad4edf92e547314a8cd6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fca26d71eec94120ae4585ded073cf13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a44961966b7400f87f36b3f80824a2d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28fb812eca7b46f48709d9c46e08dbe6",
              "IPY_MODEL_ecd3a66cd10d4e48a87dd6b7728bfeb9"
            ]
          }
        },
        "6a44961966b7400f87f36b3f80824a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28fb812eca7b46f48709d9c46e08dbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5ebec4f1d43c4ef38aa897aede3ec318",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed17e9ac13084c799eb57861b728f87b"
          }
        },
        "ecd3a66cd10d4e48a87dd6b7728bfeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e18ed3db642a4a35ade4d8e8ddc72a35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:18&lt;00:00, 273.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f65c9b68339b442e80417cafb5217a0a"
          }
        },
        "5ebec4f1d43c4ef38aa897aede3ec318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed17e9ac13084c799eb57861b728f87b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e18ed3db642a4a35ade4d8e8ddc72a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f65c9b68339b442e80417cafb5217a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3z4JFhlFajm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229b426f-e6d3-4ce3-f032-a0c40d3818dc"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed\n",
        "RAND = 0\n",
        "torch.manual_seed(RAND)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fee797417d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbEVRN9e9o8E"
      },
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK35tkv6FduV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835,
          "referenced_widgets": [
            "62f1b37a2bd4486ca3208649dc188db1",
            "1432b72627b243aa8fcc38b238d29026",
            "bc2c53448819447c91bafc4f7c05a6ca",
            "d965f76a15ad4ce2841dabe13d53ee3a",
            "7d5c467c17234b1391ab29f328fa739f",
            "66f1e729d303408586c3ee7162447542",
            "dcce2be355b5464596212e32e31fd1cb",
            "915c54a9ceae470e9f9b872813e6aa59",
            "0a518a5a070e4b11a1ec0e400e36127d",
            "221169360e024a5d9549bd71d3659c75",
            "ac18fa24fb43413bb57156666c9f4221",
            "cbf408df5b54491b90f11c673880e192",
            "9123ae8f33484115804b7341a88c3970",
            "8ad50b1dc39f4bd9a15974a7ba6eb7ce",
            "d765e3b8e1704342b16d1790998ab05b",
            "4a11f045e61943d9af4c5a00ab732163",
            "795059627a4542a088d677993caf6e69",
            "cdd7b55c1e5945deafcc5158d48368b3",
            "4fdc7b18d55844f7b591ed297cccbff3",
            "009ed755de634f99a0cb84989b3a09af",
            "5b545fe508f64d79a8c4bf1f9f54fd97",
            "c338b3a76d1d4fc7a06add09acb83a36",
            "67b677369a9648f78ca1e7cf70e48f1d",
            "4dd8db42caad4edf92e547314a8cd6ef",
            "fca26d71eec94120ae4585ded073cf13",
            "6a44961966b7400f87f36b3f80824a2d",
            "28fb812eca7b46f48709d9c46e08dbe6",
            "ecd3a66cd10d4e48a87dd6b7728bfeb9",
            "5ebec4f1d43c4ef38aa897aede3ec318",
            "ed17e9ac13084c799eb57861b728f87b",
            "e18ed3db642a4a35ade4d8e8ddc72a35",
            "f65c9b68339b442e80417cafb5217a0a"
          ]
        },
        "outputId": "bed202ce-b7ab-4ef8-b51a-2f66c0f064ca"
      },
      "source": [
        "# MNIST\n",
        "train_dataset = MNIST(root='../data', \n",
        "                      train=True, \n",
        "                      transform=transforms.ToTensor(),  \n",
        "                      download=True)\n",
        "\n",
        "test_dataset = MNIST(root='../data', \n",
        "                     train=False, \n",
        "                     transform=transforms.ToTensor(),\n",
        "                     download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62f1b37a2bd4486ca3208649dc188db1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a518a5a070e4b11a1ec0e400e36127d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "795059627a4542a088d677993caf6e69",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fca26d71eec94120ae4585ded073cf13",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaFmHZ574Mdu",
        "outputId": "700eea91-e8af-4c1a-85a4-90402243aab7"
      },
      "source": [
        "print(train_dataset.data.shape)\n",
        "print(train_dataset.targets.shape)\n",
        "print(test_dataset.data.shape)\n",
        "print(test_dataset.targets.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n",
            "torch.Size([10000, 28, 28])\n",
            "torch.Size([10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnH3fEUMBUP-",
        "outputId": "74ca845f-1574-49e7-cd64-755156358544"
      },
      "source": [
        "print(train_dataset[0][0].min())\n",
        "print(train_dataset[0][0].max())\n",
        "# Already scaled to 0-1\n",
        "print(torch.unique(train_dataset.targets))\n",
        "# Targets in their 0-9 form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MVy-_vCW8h1"
      },
      "source": [
        "# To reduce debugging time\n",
        "# train_dataset.targets = train_dataset.targets[:10000]\n",
        "# train_dataset.data = train_dataset.data[:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHElIq-o9lvX"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7N8fPWEFW_I"
      },
      "source": [
        "def random_search(hyperparameters, n_samples, RAND=None):\n",
        "    keys_to_permut = []\n",
        "    vals_to_permut = []\n",
        "    for key, val in hyperparameters.items():\n",
        "        if type(val) is tuple:\n",
        "            keys_to_permut.append(key)\n",
        "            vals_to_permut.append(val)\n",
        "\n",
        "    vals_permut = itertools.product(*vals_to_permut)\n",
        "    random.seed(RAND)\n",
        "    samples = random.sample(list(vals_permut), n_samples)\n",
        "\n",
        "    return samples, keys_to_permut\n",
        "\n",
        "\n",
        "def set_hyperparameters_tune(config, tune, keys_iter):\n",
        "    # Sets hyperparamters to selected tuning set\n",
        "    for key in keys_iter:\n",
        "        if key in config:\n",
        "            config[key] = tune[key]\n",
        "        else:\n",
        "            raise ValueError(\"Key not in config dict\")\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def train_val_split(data, stratify=None, n_folds=1, RAND=None):\n",
        "    \"\"\"\n",
        "    Uses StratifiedKFold for CV\n",
        "    Returns an iterable [{train1, test1}, ...] for each split\n",
        "    \"\"\"\n",
        "\n",
        "    it = []\n",
        "\n",
        "    if type(n_folds) is float:\n",
        "        # Loop through each class and multiply by the percentage to create the valid set\n",
        "        # Get classes for stratified sampling\n",
        "        classes = torch.unique(stratify)\n",
        "\n",
        "        # Get index for each class\n",
        "        test_index = []\n",
        "        for c in classes:\n",
        "            i_c = np.asarray((stratify == c).numpy()).nonzero()[0]\n",
        "            class_test_size = int(np.round(len(i_c) * n_folds))\n",
        "            test_index += list(i_c[random.sample(range(0, len(i_c)), class_test_size)])\n",
        "\n",
        "        train_index = np.arange(data.targets.shape[0])\n",
        "        train_index = np.delete(train_index, test_index)\n",
        "\n",
        "        it = [{'train': Subset(data, train_index), 'test': Subset(data, test_index)}]\n",
        "    else:\n",
        "        kf = StratifiedKFold(n_splits=n_folds, random_state=RAND, shuffle=True)\n",
        "        \n",
        "        for train_index, test_index in kf.split(np.zeros(len(data)), stratify):\n",
        "            it.append({'train': Subset(data, train_index), 'test': Subset(data, test_index)})\n",
        "\n",
        "    return it\n",
        "\n",
        "\n",
        "def kfoldCV(splits, config, plot_acc=False):\n",
        "    all_train_acc = []\n",
        "    all_val_acc = []\n",
        "    all_metrics = []\n",
        "    max_train_length = 0\n",
        "    max_val_length = 0\n",
        "\n",
        "    if plot_acc in ['per_split', 'norms']:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        ax1.set_title('Training Accuracy')\n",
        "        if plot_acc == 'norms':\n",
        "            ax2.set_title('Average Gradient Norm')\n",
        "        else:\n",
        "            ax2.set_title('Validation Accuracy')\n",
        "    elif plot_acc == 'mean':\n",
        "        fig, ax1 = plt.subplots(1, 1)\n",
        "        ax1.set_title('Accuracy')\n",
        "\n",
        "\n",
        "    for i_split, split in enumerate(splits):\n",
        "        if config['model_type'] == 'CNN':\n",
        "            model = CNN(config).to(config['device'])\n",
        "        elif config['model_type'] == 'MLP':\n",
        "            model = MLP(config).to(config['device'])\n",
        "        else:\n",
        "            raise NotImplementedError(\"Did not implement models other than CNN or MLP\")\n",
        "        train_loader = DataLoader(split['train'], batch_size=config['batch_size'], shuffle=True)\n",
        "        test_loader = DataLoader(split['test'], batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "        train_acc, val_acc = train_pytorch(train_loader, model, config, test_loader=test_loader)\n",
        "\n",
        "        # Calculate final accuracy of fitted model\n",
        "        if config['early_stopping']:\n",
        "            # Take the early_stopping point which is the min of 1-accuracy\n",
        "            final_val_acc = min(val_acc)\n",
        "        else:\n",
        "            # The last value is the final accuracy of the model\n",
        "            final_val_acc = val_acc[-1]\n",
        "\n",
        "        # Plot loss curves    \n",
        "        if plot_acc == 'per_split' or plot_acc == 'norms':\n",
        "            ax1.plot(train_acc, label='Split ' + str(i_split))\n",
        "            ax2.plot(val_acc, label='Split ' + str(i_split))\n",
        "\n",
        "        if len(train_acc) > max_train_length:\n",
        "            max_train_length = len(train_acc)\n",
        "        if len(val_acc) > max_val_length:\n",
        "            max_val_length = len(val_acc)\n",
        "\n",
        "        all_train_acc.append(train_acc)\n",
        "        all_val_acc.append(val_acc)\n",
        "        all_metrics.append(final_val_acc)\n",
        "\n",
        "\n",
        "    # Plot loss curves\n",
        "    if plot_acc == 'mean':\n",
        "        # Pad the loss curves with NaNs so they have equal length for nanmean\n",
        "        all_train_acc = [[l[i] if i < len(l) else np.NaN for i in range(max_train_length)] for l in all_train_acc]\n",
        "        all_val_acc = [[l[i] if i < len(l) else np.NaN for i in range(max_val_length)] for l in all_val_acc]\n",
        "        train_mean_acc = np.nanmean(all_train_acc, axis=0)\n",
        "        val_mean_acc = np.nanmean(all_val_acc, axis=0)\n",
        "        train_std_acc = np.nanstd(all_train_acc, axis=0)\n",
        "        val_std_acc = np.nanstd(all_val_acc, axis=0)\n",
        "        # Plot\n",
        "        ax1.plot(train_mean_acc, label='Training Accuracy')\n",
        "        ax1.fill_between(np.arange(max_train_length), train_mean_acc + train_std_acc, train_mean_acc - train_std_acc,\n",
        "                         alpha=0.1)\n",
        "        ax1.plot(val_mean_acc, label='Validation Accuracy')\n",
        "        ax1.fill_between(np.arange(max_val_length), val_mean_acc + val_std_acc, val_mean_acc - val_std_acc, alpha=0.1)\n",
        "\n",
        "    if plot_acc and plot_acc not in ['confusion_matrix', 'per_tune']:\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Take mean CV metric\n",
        "    avg_metric = np.mean(all_metrics)\n",
        "\n",
        "    if config['max_epochs'] is not None:\n",
        "        max_epochs = max_val_length\n",
        "    else:\n",
        "        max_epochs = round(max_val_length * len(split['train']) / config['batch_size'])\n",
        "\n",
        "    return avg_metric, max_epochs, val_acc\n",
        "\n",
        "\n",
        "def tuning_loop(splits, samples, keys, config, verbose=False, plot_acc=False):\n",
        "    all_metrics = []\n",
        "    all_max_epochs = []\n",
        "    results_experiment = {}\n",
        "\n",
        "    if plot_acc == 'per_tune':\n",
        "        if len(splits) > 1:\n",
        "            raise AssertionError(\"Not clear what to plot if more than one validation set\")\n",
        "        fig, ax1 = plt.subplots(1, 1)\n",
        "        ax1.set_title('Validation Accuracy')\n",
        "        ax1.set_ylabel('Classification Error (%)')\n",
        "        ax1.set_xlabel('Epochs')\n",
        "        ax1.set_ylim(0.8, 3.0)\n",
        "\n",
        "    # Loop through each set of hyperparameters\n",
        "    for i, params in enumerate(samples):\n",
        "        # Recreate a dict format for parameters\n",
        "        tune = {key: val for key, val in zip(keys, params)}\n",
        "        config = set_hyperparameters_tune(config.copy(), tune, keys)\n",
        "\n",
        "        # initialize model\n",
        "        metric, max_epoch, val_acc = kfoldCV(splits, config, plot_acc=plot_acc)\n",
        "        all_metrics.append(metric)\n",
        "        all_max_epochs.append(max_epoch)\n",
        "\n",
        "        # Plot loss curves\n",
        "        if plot_acc == 'per_tune':\n",
        "            ax1.plot(np.array(val_acc)*100, label='Model ' + str(i))\n",
        "\n",
        "        # Print metric\n",
        "        if verbose:\n",
        "            print(\"\\nHyperparameters:\")\n",
        "            print(config)\n",
        "            print(\"Accuracy: %.4g\" % (all_metrics[-1],))\n",
        "            results_experiment[\", \".join(\"=\".join((str(k), str(v))) for k, v in config.items())] = all_metrics[-1]\n",
        "\n",
        "\n",
        "    if plot_acc == 'per_tune':\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"per_tune_dropout_vs_dropout_maxnorm.png\")\n",
        "        plt.show()\n",
        "\n",
        "    i_best = np.argmin(all_metrics)\n",
        "    best_samples = samples[i_best]\n",
        "    # Recreate a dict format for parameters\n",
        "    best_params = {key: val for key, val in zip(keys, best_samples)}\n",
        "    best_params = set_hyperparameters_tune(config.copy(), best_params, keys)\n",
        "    # Add max early stop epoch as the new best epoch parameter and turn early stopping to False\n",
        "    best_params['early_stopping'] = False\n",
        "    if best_params['max_epochs'] is not None:\n",
        "        best_params['max_epochs'] = all_max_epochs[i_best]\n",
        "    else:\n",
        "        best_params['max_steps'] = all_max_epochs[i_best]\n",
        "    best_metric = np.min(all_metrics)\n",
        "\n",
        "    if verbose:\n",
        "        print('\\nBest hyperparameters:')\n",
        "        print(best_params)\n",
        "        print('Best average validation metric: %.4g' % (best_metric,))\n",
        "        pd.DataFrame(results_experiment.items()).to_excel(\"table9_l2.xlsx\")\n",
        "\n",
        "    return best_params, best_metric\n",
        "\n",
        "\n",
        "def train_test(train_dataset, test_dataset, config, RAND, k_folds=5, random_search_samples=50, verbose=False, plot_acc=False):\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    config['device'] = device\n",
        "    \n",
        "    # Load K-fold splits\n",
        "    if k_folds is not None:\n",
        "        splits = train_val_split(train_dataset, stratify=train_dataset.targets, n_folds=k_folds, RAND=RAND)\n",
        "\n",
        "        print(\"Tuning MLP on training set:\")\n",
        "        samples, keys = random_search(config, n_samples=random_search_samples, RAND=RAND)\n",
        "        best_params, best_metric = tuning_loop(splits, samples, keys, config, verbose=verbose, plot_acc=plot_acc)\n",
        "    else:\n",
        "        best_params = config\n",
        "\n",
        "    # Final test set using best hyperparameters\n",
        "    if best_params['model_type'] == 'CNN':\n",
        "        model = CNN(best_params).to(config['device'])\n",
        "    elif best_params['model_type'] == 'MLP':\n",
        "        model = MLP(best_params).to(config['device'])\n",
        "    else:\n",
        "        raise NotImplementedError(\"Did not implement models other than CNN or MLP\")\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                       batch_size=best_params['batch_size'],\n",
        "                                       shuffle=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=best_params['batch_size'],\n",
        "                                              shuffle=False)\n",
        "\n",
        "    train_pytorch(train_loader, model, best_params)\n",
        "\n",
        "    # Calculate final accuracy of fitted model\n",
        "    model.eval()\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    with torch.no_grad():\n",
        "        y_pred_all = []\n",
        "        y_true_all = []\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(config['device'])\n",
        "            y_batch = y_batch.to(config['device'])\n",
        "\n",
        "            y_true_all += y_batch.cpu().numpy().tolist()\n",
        "            y_pred = model(X_batch)\n",
        "\n",
        "            y_pred_prob = softmax(y_pred)\n",
        "            y_pred_tag = torch.max(y_pred_prob, dim=1).indices\n",
        "\n",
        "            y_pred_all += y_pred_tag.cpu().numpy().tolist()\n",
        "\n",
        "        test_acc = evaluate_acc(np.array(y_true_all), np.array(y_pred_all))\n",
        "\n",
        "    print('Test accuracy using best hyperparameters: %.4g' % (test_acc,))\n",
        "    results_experiment = {}\n",
        "    results_experiment[\", \".join(\"=\".join((str(k), str(v))) for k, v in best_params.items())] = test_acc\n",
        "    pd.DataFrame(results_experiment.items()).to_excel(\"table9_l2_test.xlsx\")\n",
        "\n",
        "class EarlyStop():\n",
        "    def __init__(self, patience=10, reverse=False):\n",
        "        # Set reverse to True if a better metric is a larger one, False if opposite\n",
        "        self.best_metric = None\n",
        "        self.best_weights = None\n",
        "        self.reverse = reverse\n",
        "        self.patience = patience\n",
        "        self.latency = 0\n",
        "\n",
        "    def update_metric(self, valid_metric, weights):\n",
        "        update = False\n",
        "        early_stop = False\n",
        "        if self.best_metric is None:\n",
        "            self.best_metric = valid_metric\n",
        "            self.best_weights = weights\n",
        "            update = True\n",
        "        elif self.reverse:\n",
        "            if self.best_metric < valid_metric:\n",
        "                self.best_metric = valid_metric\n",
        "                self.best_weights = weights\n",
        "                update = True\n",
        "        else:\n",
        "            if self.best_metric > valid_metric:\n",
        "                self.best_metric = valid_metric\n",
        "                self.best_weights = weights\n",
        "                update = True\n",
        "        if not update:\n",
        "            if self.latency == self.patience:\n",
        "                early_stop = True  # Early stop\n",
        "            else:\n",
        "                self.latency += 1  # Continue until patience\n",
        "        else:\n",
        "            early_stop = False  # Since just updated best_metric, continue until patience\n",
        "            self.latency = 0  # Reset latency\n",
        "        return early_stop\n",
        "\n",
        "\n",
        "def evaluate_acc(true_y, target_y):\n",
        "    # The dropout paper presents errors, not accuracy, so do 1-accuracy\n",
        "    return 1 - np.sum(target_y == true_y)/true_y.shape[0]\n",
        "\n",
        "\n",
        "def max_norm(model, max_val=3):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'bias' not in name:\n",
        "                norm = param.norm(2, dim=0, keepdim=True)  # Compute L2 norm for all weights\n",
        "                scale = torch.clamp(norm, 0, max_val)  # Clamp norms to desired max val\n",
        "                param *= (scale / norm)  # Rescale the parameters so the new norm = max_val\n",
        "\n",
        "\n",
        "def train_pytorch(train_loader, model, config, test_loader=None):\n",
        "    if config['epsilon_decay']:\n",
        "        lr = config['base_epsilon']\n",
        "    else:\n",
        "        lr = config['learning_rate']\n",
        "\n",
        "    if config['momentum_decay']:\n",
        "        momentum = config['initial_momentum']\n",
        "    else:\n",
        "        momentum = config['momentum']\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=config['L2_regularization_param'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    # Setup Early Stopping\n",
        "    early_stop = EarlyStop(patience=10, reverse=True)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    running_ap = 0.0\n",
        "    beta = 0.9\n",
        "\n",
        "    epoch = 0 # Epoch count\n",
        "    step = 0  # Step count\n",
        "\n",
        "    if config['max_epochs'] is not None and config['max_steps'] is not None:\n",
        "        raise AssertionError(\"Need to specify one or the other, but not both.\")\n",
        "\n",
        "    if config['max_epochs'] is not None:\n",
        "        max_it = config['max_epochs']\n",
        "    else:\n",
        "        max_it = config['max_steps']\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    pbar = tqdm(total=max_it)\n",
        "    while i <= max_it:\n",
        "\n",
        "        # --- Train ---\n",
        "        model.train()\n",
        "        y_pred_all = []\n",
        "        y_true_all = []\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(config['device'])\n",
        "            y_batch = y_batch.to(config['device'])\n",
        "\n",
        "            y_true_all += y_batch.cpu().numpy().tolist()\n",
        "            y_batch = y_batch.long()\n",
        "            y_pred = model(X_batch)\n",
        "\n",
        "            y_pred_prob = softmax(y_pred)\n",
        "            y_pred_tag = torch.max(y_pred_prob, dim=1).indices\n",
        "\n",
        "            y_pred_all += y_pred_tag.cpu().numpy().tolist()\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            # Add L1 \"towards the end of training\"\n",
        "            if config['L1_regularization_param'] > 0:\n",
        "                if step > config['L1_start_step']:\n",
        "                    L1 = torch.tensor(0., requires_grad=True)\n",
        "                    for name, param in model.named_parameters():\n",
        "                        if 'weight' in name:\n",
        "                            L1 = L1 + torch.norm(param, 1)\n",
        "\n",
        "                    loss = loss + config['L1_regularization_param'] * L1\n",
        "\n",
        "            # Add a KL sparsity penalty\n",
        "            if config['kl_sparsity_beta']:\n",
        "                loss = loss + config['kl_sparsity_beta'] * model.kl_div\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if config['max_norm'] is not None:\n",
        "                max_norm(model, config['max_norm'])\n",
        "\n",
        "            if config['epsilon_decay']:\n",
        "                epsilon = get_epsilon(step, config)\n",
        "                update_optim(optimizer, epsilon, 'lr')\n",
        "            \n",
        "            if config['momentum_decay']:\n",
        "                momentum = get_momentum(step, config)\n",
        "                update_optim(optimizer, momentum, 'momentum')\n",
        "\n",
        "            step += 1\n",
        "            if config['max_steps'] is not None and step >= config['max_steps']:\n",
        "                break\n",
        "\n",
        "        train_acc.append(evaluate_acc(np.array(y_true_all), np.array(y_pred_all)))\n",
        "\n",
        "        # --- Validate ---\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if test_loader is not None:\n",
        "                y_pred_all = []\n",
        "                y_true_all = []\n",
        "                for X_batch, y_batch in test_loader:\n",
        "                    X_batch = X_batch.to(config['device'])\n",
        "                    y_batch = y_batch.to(config['device'])\n",
        "\n",
        "                    y_true_all += y_batch.cpu().numpy().tolist()\n",
        "                    y_pred = model(X_batch)\n",
        "\n",
        "                    y_pred_prob = softmax(y_pred)\n",
        "                    y_pred_tag = torch.max(y_pred_prob, dim=1).indices\n",
        "\n",
        "                    y_pred_all += y_pred_tag.cpu().numpy().tolist()\n",
        "\n",
        "                val_acc.append(evaluate_acc(np.array(y_true_all), np.array(y_pred_all)))\n",
        "\n",
        "                # Update moving average\n",
        "                if epoch == 0:\n",
        "                    running_ap = val_acc[-1]\n",
        "                else:\n",
        "                    running_ap = beta * running_ap + (1 - beta) * val_acc[-1]\n",
        "\n",
        "                # early stop\n",
        "                if config['early_stopping']:\n",
        "                    if early_stop.update_metric(running_ap, model.state_dict()):\n",
        "                        # Save best weights\n",
        "                        model.load_state_dict(early_stop.best_weights)\n",
        "                        break\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "        if config['max_epochs'] is not None:\n",
        "            pbar.update(n=epoch - i)\n",
        "            i = epoch\n",
        "        else:\n",
        "            pbar.update(n=step - i)\n",
        "            i = step\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    return train_acc, val_acc\n",
        "\n",
        "\n",
        "def kl_div(r, r_hat, epsilon=1e-6):\n",
        "    r_hat = torch.mean(torch.clamp(r_hat, min=epsilon, max=1 - epsilon), 0)  # maximum to constrain rho between 0 and 1\n",
        "    r = torch.Tensor([r] * len(r_hat)).to(r_hat.device)\n",
        "    return torch.sum(r * torch.log(r / r_hat) + (1 - r) * torch.log((1 - r) / (1 - r_hat)))\n",
        "\n",
        "\n",
        "def update_optim(optim, param_val, param_name):\n",
        "    \"\"\"Updates the momentum/lr manually for each parameter group in the optimizer\"\"\"\n",
        "    for g in optim.param_groups:\n",
        "          g[param_name] = param_val\n",
        "\n",
        "def get_momentum(step, config):\n",
        "    \"\"\"\n",
        "    ***Adapted from: https://github.com/nitishsrivastava/deepnet/blob/f4e4ff207923e01552c96038a1e2c29eb5d16160/deepnet/parameter.py\n",
        "    \"\"\"\n",
        "    momentum = config['final_momentum'] - (config['final_momentum'] - config['initial_momentum'])*np.exp(-float(step)/config['momentum_change_steps'])\n",
        "    \n",
        "    return momentum\n",
        "\n",
        "def get_epsilon(step, config):\n",
        "    \"\"\"\n",
        "    ***Adapted from: https://github.com/nitishsrivastava/deepnet/blob/f4e4ff207923e01552c96038a1e2c29eb5d16160/deepnet/parameter.py\n",
        "    \"\"\"\n",
        "    epsilon = config['base_epsilon']\n",
        "    if config['epsilon_decay'] == 'INVERSE_T':\n",
        "      epsilon = config['base_epsilon'] / (1 + float(step) / config['epsilon_decay_half_life'])\n",
        "    elif config['epsilon_decay'] == 'EXPONENTIAL':\n",
        "      epsilon = config['base_epsilon'] / np.power(2, float(step) / config['epsilon_decay_half_life'])\n",
        "\n",
        "    return epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlwaukvj9L_X"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4b9L63W9Jos"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # Set up layers\n",
        "        self.h_layer = nn.ModuleList([nn.Linear(config['D_in'], d) if i == 0 else nn.Linear(config['D_h'][i - 1], d) for i, d in enumerate(config['D_h'])])\n",
        "        self.out_layer = nn.Linear(config['D_h'][-1], config['D_out'])\n",
        "\n",
        "        # Put each dropout layer in a ModuleList to enable different p for each layer\n",
        "        dropout_list = []\n",
        "        for p in config['dropout']:\n",
        "            dropout_list.append(nn.Dropout(p=p))\n",
        "        self.dropout_layers = nn.ModuleList(dropout_list)\n",
        "\n",
        "        self.activation = eval('nn.' + config['activation'])()\n",
        "\n",
        "        self.kl_sparsity = config['kl_sparsity']\n",
        "        self.kl_div = torch.tensor(0., requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        self.kl_div = torch.tensor(0., requires_grad=True)\n",
        "        for i, l in enumerate(self.h_layer):\n",
        "            x = self.activation(l(x))\n",
        "            if self.kl_sparsity is not None:\n",
        "               self.kl_div = self.kl_div + kl_div(self.kl_sparsity, x)\n",
        "\n",
        "            if i > len(self.dropout_layers) - 1:\n",
        "                # Assume the last value of the dropout list is meant for all layers\n",
        "                x = self.dropout_layers[-1](x)\n",
        "            else:\n",
        "                # Use the layer specific dropout value\n",
        "                x = self.dropout_layers[i](x)\n",
        "\n",
        "        out = self.out_layer(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IijG-q659QRW"
      },
      "source": [
        "# Experiment Setup (Example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Jij2qXEZFsIN",
        "outputId": "6b6a4686-06d3-46ba-af1b-5afab8da1056"
      },
      "source": [
        "# Set hyperparameters\n",
        "config = {\n",
        "    'batch_size': 264,\n",
        "    'early_stopping': True,\n",
        "    'max_epochs': 500,\n",
        "    'momentum': 0.95,\n",
        "    'momentum_decay': True, # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000, # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': 0.1, # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL', # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': 0.1,\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.5, 0.9, 0.9], # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': X_train.shape[1],\n",
        "    'D_h': ([0],), # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,), # Number of classes\n",
        "    'max_norm': (2,),  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'CNN' # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=0.1, random_search_samples=1, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3689249f9c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Implement your models with each layer's p in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m'activation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;34m'D_in'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;34m'D_h'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m'D_out'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu-f5BcbKHPm"
      },
      "source": [
        "# Table 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY19-_KzKGCc"
      },
      "source": [
        "# Table 2: neural network ReLU\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([800, 800], [800, 800, 800], [1024, 1024], [1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=12, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuZSk_PUpgrQ"
      },
      "source": [
        "# Table 2: neural network Sigmoid\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'Sigmoid',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([800, 800], [800, 800, 800], [1024, 1024], [1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=12, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edpN9XlSpt7H"
      },
      "source": [
        "# Table 2: neural network with dropout ReLU\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.2, 0.5],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([800, 800], [800, 800, 800], [1024, 1024], [1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=12, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6zKdykxp7k-"
      },
      "source": [
        "# Table 2: neural network with dropout Sigmoid\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.2, 0.5],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'Sigmoid',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([800, 800], [800, 800, 800], [1024, 1024], [1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=12, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_6FJpV2p_up"
      },
      "source": [
        "# Table 2: neural network with dropout + max-norm ReLU\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.2, 0.5],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([800, 800], [800, 800, 800], [1024, 1024], [1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': (4, 3.5, 3),  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=24, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th5o_UQ3XVVP"
      },
      "source": [
        "# Figure 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf8_Y5UHXQsM"
      },
      "source": [
        "# Figure 4 v1: ReLU\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': 0.01,\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': ([0.2, 0.5], [0.0]),  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([1024, 1024], [1024, 1024, 1024], [1024, 1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048], [2048, 2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=12, verbose=True, plot_acc='per_tune')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flq-CHw22Npv"
      },
      "source": [
        "# Figure 4 v2: Sigmoid\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': 0.01,\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': ([0.2, 0.5], [0.0]),  # Implement your models with each layer's p in a list\n",
        "    'activation': 'Sigmoid',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([1024, 1024], [1024, 1024, 1024], [1024, 1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048], [2048, 2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,),  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=12, verbose=True, plot_acc='per_tune')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZrLTM_O1u8w"
      },
      "source": [
        "# Figure 4 extra: L2 alone with 0.01\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': 0.01,\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': 0.0001,\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([1024, 1024], [1024, 1024, 1024], [1024, 1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048], [2048, 2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=6, verbose=True, plot_acc='per_tune')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNYDqZDi34jC"
      },
      "source": [
        "# Figure 4 extra: Dropout alonewith 0.01\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': 0.01,\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': 0.0,\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.2, 0.5],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': ([1024, 1024], [1024, 1024, 1024], [1024, 1024, 1024, 1024], [2048, 2048], [2048, 2048, 2048], [2048, 2048, 2048, 2048]),  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=6, verbose=True, plot_acc='per_tune')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHNsgbiZaJV1"
      },
      "source": [
        "# Table 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ92IKUManpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8597ecb2-bb86-4ca4-d352-a7ab24ee0d11"
      },
      "source": [
        "# L2\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': [1024, 1024, 2048],  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=8, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tuning MLP on training set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [30:03, 110.91it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.01, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.001, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [29:56, 111.35it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.01, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.0001, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.0203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [29:54, 111.43it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.1, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.0001, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.0156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [30:03, 110.92it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.1, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.1, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.8876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [30:08, 110.57it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.1, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.001, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.0331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [30:01, 111.00it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.01, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.01, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.0449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "200001it [29:55, 111.40it/s]                            \n",
            "  0%|          | 0/200000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters:\n",
            "{'batch_size': 100, 'early_stopping': False, 'max_epochs': None, 'max_steps': 200000, 'momentum': None, 'momentum_decay': True, 'initial_momentum': 0.5, 'final_momentum': 0.95, 'momentum_change_steps': 20000, 'learning_rate': None, 'epsilon_decay': 'EXPONENTIAL', 'base_epsilon': 0.1, 'epsilon_decay_half_life': 100000, 'L2_regularization_param': 0.01, 'dropout': [0.0], 'activation': 'ReLU', 'D_in': 784, 'D_h': [1024, 1024, 2048], 'D_out': 10, 'max_norm': None, 'model_type': 'MLP', 'device': device(type='cuda', index=0)}\n",
            "Accuracy: 0.06909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|███▏      | 62500/200000 [09:22<20:34, 111.42it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcPqL292avsN"
      },
      "source": [
        "# L2 + L1 applied towards the end of training\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_regularization_param': (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': [1024, 1024, 2048],  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=24, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7cBYrk2avpj"
      },
      "source": [
        "# L2 + KL sparsity\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': 0.05,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': (0.1, 0.01, 0.001, 0.0001),  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': [1024, 1024, 2048],  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=32, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99mTy6wAavmq"
      },
      "source": [
        "# Max-norm\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': 0.0, # (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.0],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': [1024, 1024, 2048],  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': (4, 3.5, 3),  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=6, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rsq6GNUavhT"
      },
      "source": [
        "# Dropout + L2\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': (0.1, 0.01, 0.001, 0.0001), # (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.2, 0.5],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': [1024, 1024, 2048],  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': None,  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=8, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuvNMPI4avZa"
      },
      "source": [
        "# Dropout + Max-norm\n",
        "config = {\n",
        "    'batch_size': 100,\n",
        "    'early_stopping': False,\n",
        "    'max_epochs': None,  # If None need to specify max_steps\n",
        "    'max_steps': 200000,  # If None need to specify max_epochs\n",
        "    'momentum': None,\n",
        "    'momentum_decay': True,  # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': 0.5,\n",
        "    'final_momentum': 0.95,\n",
        "    'momentum_change_steps': 20000,  # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': None,  # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': 'EXPONENTIAL',  # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1, 0.01),\n",
        "    'epsilon_decay_half_life': 100000,\n",
        "    'L2_regularization_param': 0.0, # (0.1, 0.01, 0.001, 0.0001),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': [0.2, 0.5],  # Implement your models with each layer's p in a list\n",
        "    'activation': 'ReLU',\n",
        "    'D_in': 784,\n",
        "    'D_h': [1024, 1024, 2048],  # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': 10,  # Number of classes\n",
        "    'max_norm': (4, 3.5, 3),  # (4, 3, 2, 1, 0.5, None) # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'MLP'  # MLP or CNN\n",
        "}\n",
        "train_test(train_dataset, test_dataset, config, RAND, k_folds=1/6, random_search_samples=6, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYoOI9olgAiL"
      },
      "source": [
        "# Additional Results 1 - Ablation study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2WP-migwJJ"
      },
      "source": [
        "**MODEL A**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptO1VPJmgGW0"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 1024\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "class CustomDropout(nn.Module):\n",
        "    def __init__(self, p=0.5, prob_mask_prop=0.5,  method='standard'):\n",
        "        super(CustomDropout, self).__init__()\n",
        "        #prob_mask_prop = probabilistic mask proportion of the layer (0 to 1)\n",
        "        self.p = p\n",
        "        self.prob_mask_prop = prob_mask_prop\n",
        "        self.method = method\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        batch_s,wide_layer = input.size()\n",
        "        prob_layer_size = round(wide_layer*self.prob_mask_prop)\n",
        "\n",
        "        if self.training:\n",
        "          if self.method == 'standard':\n",
        "            prob_mask = torch.distributions.bernoulli.Bernoulli(1-self.p).sample([batch_s,prob_layer_size  ]).to(device='cuda')\n",
        "            prob_mask = prob_mask* (1.0/(1-self.p))\n",
        "          else:\n",
        "            prob_mask = torch.randn(batch_s,prob_layer_size, requires_grad=False).to(device='cuda')   \n",
        "            prob_mask = prob_mask* (self.p/(1-self.p)) + 1 \n",
        "          fix_part = torch.ones(batch_s, wide_layer - prob_layer_size, requires_grad=False).to(device='cuda')\n",
        "          final_mask = torch.cat((prob_mask, fix_part), 1)\n",
        "\n",
        "          return input * final_mask\n",
        "        else:\n",
        "          return input\n",
        "\n",
        "    def __repr__(self):\n",
        "        prob_mask_prop_str = ''+str(self.prob_mask_prop) if self.prob_mask_prop > 0 else ''\n",
        "        return self.__class__.__name__ + '(' + 'p=' + str(self.p) + ' , prob_mask_prop=' + prob_mask_prop_str + ' , method=' +  str(self.method) + ')'\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tYjmUr-gGTf"
      },
      "source": [
        "class MLP1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes , method='standard', prob_mask_prop = 1 ):\n",
        "        super(MLP1, self).__init__()\n",
        "\n",
        "        self.hidden_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer1.weight, gain=1, mode='fan_in')\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer2.weight, gain=1,  mode='fan_in')\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer3.weight, gain=1, mode='fan_in')\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "        #torch.nn.init.kaiming_normal(self.output_layer.weight, gain=1, mode='fan_in')\n",
        "        #self.custom_dropout = CustomDropout(p=0.5, prob_mask_prop=1 )\n",
        "        #self.dropout_input = CustomDropout(p=0.2, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        self.dropout_HL = CustomDropout(p=0.5, prob_mask_prop=prob_mask_prop , method=method ) \n",
        "        self.dropout_input = CustomDropout(p=0.2, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        #self.dropout_HL = nn.Dropout(0.5) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = self.dropout_input(x)      \n",
        "        out = F.relu(self.hidden_layer1(x))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer2(out))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer3(out))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhrUTp8BgGPX"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tsHQgFHgaAa"
      },
      "source": [
        "# Varying the mask size for custom dropout\n",
        "\n",
        "list_proportion_prob = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "for prob in list_proportion_prob:\n",
        "  model1 = MLP1(input_size, hidden_size, num_classes, method='standard', prob_mask_prop = prob)\n",
        "\n",
        "  # If GPU is available, move the model to GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model1.to(device)\n",
        "\n",
        "  print(f\"Network Architecture: \\n {model1}\")\n",
        "  #print(f\"The model is at : {device}\")\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "  initial_momentum = 0.50\n",
        "  final_momentum = 0.95\n",
        "  momentum_change_steps = 1000\n",
        "\n",
        "  base_epsilon = 0.02\n",
        "  epsilon_decay_half_life = 5000\n",
        "\n",
        "  step = 0  # Step count\n",
        "\n",
        "  losses = []\n",
        "  model1.train()\n",
        "  for epoch in range(20):\n",
        "      epoch_loss = []\n",
        "\n",
        "      # Iterate over data\n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):  \n",
        "        #move tensor to the same device (CPU/GPU) as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "      \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()  \n",
        "      \n",
        "        # forward + loss calc + backward + step\n",
        "        outputs = model1(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #test_lr_scheduler.step()\n",
        "      \n",
        "        max_norm(model, 3.5)\n",
        "\n",
        "        momentum = final_momentum - (final_momentum - initial_momentum)*np.exp(-float(step)/momentum_change_steps)\n",
        "        update_optim(optimizer, momentum, 'momentum')\n",
        "\n",
        "        #epsilon = base_epsilon / (1 + float(step) / epsilon_decay_half_life)\n",
        "        epsilon = base_epsilon / np.power(2, float(step) / epsilon_decay_half_life)\n",
        "        update_optim(optimizer, epsilon, 'lr')\n",
        "      \n",
        "\n",
        "        step += 1\n",
        "\n",
        "        #if batch_idx % 600 == 0:\n",
        "        #  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "      losses.append(np.mean(epoch_loss))\n",
        "\n",
        "  plt.plot(np.arange(len(losses)), losses, 'r')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          images = images.view(-1, 28*28)\n",
        "          outputs = model1(images)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dyDYtZHgZ9u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f1271bb-28c2-4f80-b4f3-700d423fdc20"
      },
      "source": [
        "# Fixing the effective dropout rate and varying the layer mask size\n",
        "\n",
        "list_proportion_prob = [0.55, 0.65, 0.75, 0.85, 0.95, 1]\n",
        "\n",
        "for prob in list_proportion_prob:\n",
        "  dropoutHL = 0.35/prob\n",
        "  dropout_input = 0.14/prob\n",
        "\n",
        "  class MLP1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes , method='standard', prob_mask_prop = 1 ):\n",
        "        super(MLP1, self).__init__()\n",
        "\n",
        "        self.hidden_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer1.weight, a=1, mode='fan_in')\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer2.weight, a=1,  mode='fan_in')\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer3.weight, a=1, mode='fan_in')\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "        #torch.nn.init.kaiming_normal(self.output_layer.weight, a=1, mode='fan_in')\n",
        "        #self.custom_dropout = CustomDropout(p=0.5, prob_mask_prop=1 )\n",
        "        self.dropout_input = CustomDropout(p=dropout_input, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        self.dropout_HL = CustomDropout(p=dropoutHL, prob_mask_prop=prob_mask_prop , method=method ) \n",
        "        #self.dropout_input = nn.Dropout(0.2)\n",
        "        #self.dropout_HL = nn.Dropout(0.5) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = self.dropout_input(x)      \n",
        "        out = F.relu(self.hidden_layer1(x))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer2(out))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer3(out))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "  model1 = MLP1(input_size, hidden_size, num_classes, method='standard', prob_mask_prop = prob)\n",
        "\n",
        "  # If GPU is available, move the model to GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model1.to(device)\n",
        "\n",
        "  print(f\"Network Architecture: \\n {model1}\")\n",
        "  #print(f\"The model is at : {device}\")\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "  initial_momentum = 0.50\n",
        "  final_momentum = 0.95\n",
        "  momentum_change_steps = 1000\n",
        "\n",
        "  base_epsilon = 0.02\n",
        "  epsilon_decay_half_life = 5000\n",
        "\n",
        "  step = 0  # Step count\n",
        "\n",
        "  losses = []\n",
        "  model1.train()\n",
        "  for epoch in range(20):\n",
        "      epoch_loss = []\n",
        "\n",
        "      # Iterate over data\n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):  \n",
        "        #move tensor to the same device (CPU/GPU) as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "      \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()  \n",
        "      \n",
        "        # forward + loss calc + backward + step\n",
        "        outputs = model1(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #test_lr_scheduler.step()\n",
        "      \n",
        "        max_norm(model1, 3.5)\n",
        "\n",
        "        momentum = final_momentum - (final_momentum - initial_momentum)*np.exp(-float(step)/momentum_change_steps)\n",
        "        update_optim(optimizer, momentum, 'momentum')\n",
        "\n",
        "        #epsilon = base_epsilon / (1 + float(step) / epsilon_decay_half_life)\n",
        "        epsilon = base_epsilon / np.power(2, float(step) / epsilon_decay_half_life)\n",
        "        update_optim(optimizer, epsilon, 'lr')\n",
        "      \n",
        "\n",
        "        step += 1\n",
        "\n",
        "        #if batch_idx % 600 == 0:\n",
        "        #  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "      losses.append(np.mean(epoch_loss))\n",
        "\n",
        "  plt.plot(np.arange(len(losses)), losses, 'r')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          images = images.view(-1, 28*28)\n",
        "          outputs = model1(images)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (dropout_input): CustomDropout(p=0.2545454545454546 , prob_mask_prop=0.55 , method=standard)\n",
            "  (dropout_HL): CustomDropout(p=0.6363636363636362 , prob_mask_prop=0.55 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.47 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (dropout_input): CustomDropout(p=0.2153846153846154 , prob_mask_prop=0.65 , method=standard)\n",
            "  (dropout_HL): CustomDropout(p=0.5384615384615384 , prob_mask_prop=0.65 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.6 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (dropout_input): CustomDropout(p=0.18666666666666668 , prob_mask_prop=0.75 , method=standard)\n",
            "  (dropout_HL): CustomDropout(p=0.4666666666666666 , prob_mask_prop=0.75 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.53 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (dropout_input): CustomDropout(p=0.1647058823529412 , prob_mask_prop=0.85 , method=standard)\n",
            "  (dropout_HL): CustomDropout(p=0.4117647058823529 , prob_mask_prop=0.85 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.64 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (dropout_input): CustomDropout(p=0.1473684210526316 , prob_mask_prop=0.95 , method=standard)\n",
            "  (dropout_HL): CustomDropout(p=0.3684210526315789 , prob_mask_prop=0.95 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.65 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (dropout_input): CustomDropout(p=0.14 , prob_mask_prop=1 , method=standard)\n",
            "  (dropout_HL): CustomDropout(p=0.35 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.65 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0jd-ZwDgZ7L"
      },
      "source": [
        "# Varying the dropout rate with full max (regular dropout)\n",
        "\n",
        "list_proportion_prob = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "for prob in list_proportion_prob:\n",
        "  dropoutHL = 0.5*prob\n",
        "  dropout_input = 0.2*prob\n",
        "\n",
        "  class MLP1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes , method='standard', prob_mask_prop = 1 ):\n",
        "        super(MLP1, self).__init__()\n",
        "\n",
        "        self.hidden_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer1.weight, a=1, mode='fan_in')\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer2.weight, a=1,  mode='fan_in')\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        #torch.nn.init.kaiming_normal(self.hidden_layer3.weight, a=1, mode='fan_in')\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "        #torch.nn.init.kaiming_normal(self.output_layer.weight, a=1, mode='fan_in')\n",
        "        #self.custom_dropout = CustomDropout(p=0.5, prob_mask_prop=1 )\n",
        "        self.dropout_input = CustomDropout(p=dropout_input, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        self.dropout_HL = CustomDropout(p=dropoutHL, prob_mask_prop=prob_mask_prop , method=method ) \n",
        "        #self.dropout_input = nn.Dropout(0.2)\n",
        "        #self.dropout_HL = nn.Dropout(0.5) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = self.dropout_input(x)      \n",
        "        out = F.relu(self.hidden_layer1(x))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer2(out))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer3(out))\n",
        "        out = self.dropout_HL(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "  model1 = MLP1(input_size, hidden_size, num_classes, method='standard', prob_mask_prop = 1)\n",
        "\n",
        "  # If GPU is available, move the model to GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model1.to(device)\n",
        "\n",
        "  print(f\"Network Architecture: \\n {model1}\")\n",
        "  #print(f\"The model is at : {device}\")\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "  initial_momentum = 0.50\n",
        "  final_momentum = 0.95\n",
        "  momentum_change_steps = 1000\n",
        "\n",
        "  base_epsilon = 0.02\n",
        "  epsilon_decay_half_life = 5000\n",
        "\n",
        "  step = 0  # Step count\n",
        "\n",
        "  losses = []\n",
        "  model1.train()\n",
        "  for epoch in range(20):\n",
        "      epoch_loss = []\n",
        "\n",
        "      # Iterate over data\n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):  \n",
        "        #move tensor to the same device (CPU/GPU) as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "      \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()  \n",
        "      \n",
        "        # forward + loss calc + backward + step\n",
        "        outputs = model1(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #test_lr_scheduler.step()\n",
        "      \n",
        "        max_norm(model, 3.5)\n",
        "\n",
        "        momentum = final_momentum - (final_momentum - initial_momentum)*np.exp(-float(step)/momentum_change_steps)\n",
        "        update_optim(optimizer, momentum, 'momentum')\n",
        "\n",
        "        #epsilon = base_epsilon / (1 + float(step) / epsilon_decay_half_life)\n",
        "        epsilon = base_epsilon / np.power(2, float(step) / epsilon_decay_half_life)\n",
        "        update_optim(optimizer, epsilon, 'lr')\n",
        "      \n",
        "\n",
        "        step += 1\n",
        "\n",
        "        #if batch_idx % 600 == 0:\n",
        "        #  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "      losses.append(np.mean(epoch_loss))\n",
        "\n",
        "  plt.plot(np.arange(len(losses)), losses, 'r')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          images = images.view(-1, 28*28)\n",
        "          outputs = model1(images)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WcqlJaTg1wG"
      },
      "source": [
        "**MODEL B**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nSD79jqgn9P"
      },
      "source": [
        "class MLP1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, method='standard', prob_mask_prop = 1 ):\n",
        "        super(MLP1, self).__init__()\n",
        "\n",
        "        self.hidden_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "        self.custom_dropout_HL = CustomDropout(p=0.5, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        self.custom_dropout_input = CustomDropout(p=0.2, prob_mask_prop=prob_mask_prop, method=method )\n",
        "        #self.dropout_input = nn.Dropout(0.2)\n",
        "        #self.dropout_HL = nn.Dropout(0.5) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = self.custom_dropout_input(x)      \n",
        "        out = F.relu(self.hidden_layer1(x))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer2(out))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer3(out))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JygLGZO-gn6I"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvMNzRZign2z"
      },
      "source": [
        "list_proportion_prob = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "for prob in list_proportion_prob:\n",
        "  model1 = MLP1(input_size, hidden_size, num_classes, method='standard', prob_mask_prop = prob)\n",
        "\n",
        "  # If GPU is available, move the model to GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model1.to(device)\n",
        "\n",
        "  print(f\"Network Architecture: \\n {model1}\")\n",
        "  #print(f\"The model is at : {device}\")\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "  #initial_momentum = 0.5\n",
        "  #final_momentum = 0.95\n",
        "  #momentum_change_steps = 5000\n",
        "  step = 0  # Step count\n",
        "\n",
        "  losses = []\n",
        "  model1.train()\n",
        "  for epoch in range(50):\n",
        "      epoch_loss = []\n",
        "\n",
        "      # Iterate over data\n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):  \n",
        "        #move tensor to the same device (CPU/GPU) as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "      \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()  \n",
        "      \n",
        "        # forward + loss calc + backward + step\n",
        "        outputs = model1(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #test_lr_scheduler.step()\n",
        "\n",
        "      \n",
        "      \n",
        "        #momentum = final_momentum - (final_momentum - initial_momentum)*np.exp(-float(step)/momentum_change_steps)\n",
        "        #update_optim(optimizer, momentum, 'momentum')\n",
        "      \n",
        "\n",
        "        step += 1\n",
        "\n",
        "        #if batch_idx % 600 == 0:\n",
        "        #  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "      losses.append(np.mean(epoch_loss))\n",
        "\n",
        "  plt.plot(np.arange(len(losses)), losses, 'r')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          images = images.view(-1, 28*28)\n",
        "          outputs = model1(images)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDaaxTMkgZ4n"
      },
      "source": [
        "list_proportion_prob = [0.55, 0.65, 0.75, 0.85, 0.95, 1]\n",
        "\n",
        "for prob in list_proportion_prob:\n",
        "  dropoutHL = 0.5/prob\n",
        "  dropout_input = 0.2/prob\n",
        "  class MLP1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, method='standard', prob_mask_prop = 1 ):\n",
        "        super(MLP1, self).__init__()\n",
        "\n",
        "        self.hidden_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "        self.custom_dropout_HL = CustomDropout(p=dropoutHL, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        self.custom_dropout_input = CustomDropout(p=dropout_input, prob_mask_prop=prob_mask_prop, method=method )\n",
        "        #self.dropout_input = nn.Dropout(0.2)\n",
        "        #self.dropout_HL = nn.Dropout(0.5) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = self.custom_dropout_input(x)      \n",
        "        out = F.relu(self.hidden_layer1(x))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer2(out))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer3(out))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "  model1 = MLP1(input_size, hidden_size, num_classes, method='standard', prob_mask_prop = prob)\n",
        "\n",
        "  # If GPU is available, move the model to GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model1.to(device)\n",
        "\n",
        "  print(f\"Network Architecture: \\n {model1}\")\n",
        "  #print(f\"The model is at : {device}\")\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "  #initial_momentum = 0.5\n",
        "  #final_momentum = 0.95\n",
        "  #momentum_change_steps = 5000\n",
        "  step = 0  # Step count\n",
        "\n",
        "  losses = []\n",
        "  model1.train()\n",
        "  for epoch in range(50):\n",
        "      epoch_loss = []\n",
        "\n",
        "      # Iterate over data\n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):  \n",
        "        #move tensor to the same device (CPU/GPU) as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "      \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()  \n",
        "      \n",
        "        # forward + loss calc + backward + step\n",
        "        outputs = model1(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #test_lr_scheduler.step()\n",
        "\n",
        "      \n",
        "      \n",
        "        #momentum = final_momentum - (final_momentum - initial_momentum)*np.exp(-float(step)/momentum_change_steps)\n",
        "        #update_optim(optimizer, momentum, 'momentum')\n",
        "      \n",
        "\n",
        "        step += 1\n",
        "\n",
        "        #if batch_idx % 600 == 0:\n",
        "        #  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "      losses.append(np.mean(epoch_loss))\n",
        "\n",
        "  plt.plot(np.arange(len(losses)), losses, 'r')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          images = images.view(-1, 28*28)\n",
        "          outputs = model1(images)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJAgW2dUgZ1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f9401e-ee87-45b9-ebe2-4e3a63f35aa9"
      },
      "source": [
        "list_proportion_prob = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "for prob in list_proportion_prob:\n",
        "  dropoutHL = 0.35*prob\n",
        "  dropout_input = 0.14*prob\n",
        "  class MLP1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, method='standard', prob_mask_prop = 1 ):\n",
        "        super(MLP1, self).__init__()\n",
        "\n",
        "        self.hidden_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "        self.custom_dropout_HL = CustomDropout(p=dropoutHL, prob_mask_prop=prob_mask_prop , method=method )\n",
        "        self.custom_dropout_input = CustomDropout(p=dropout_input, prob_mask_prop=prob_mask_prop, method=method )\n",
        "        #self.dropout_input = nn.Dropout(0.2)\n",
        "        #self.dropout_HL = nn.Dropout(0.5) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = self.custom_dropout_input(x)      \n",
        "        out = F.relu(self.hidden_layer1(x))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer2(out))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = F.relu(self.hidden_layer3(out))\n",
        "        out = self.custom_dropout_HL(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "  model1 = MLP1(input_size, hidden_size, num_classes, method='standard', prob_mask_prop = 1)\n",
        "\n",
        "  # If GPU is available, move the model to GPU\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model1.to(device)\n",
        "\n",
        "  print(f\"Network Architecture: \\n {model1}\")\n",
        "  #print(f\"The model is at : {device}\")\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "  #initial_momentum = 0.5\n",
        "  #final_momentum = 0.95\n",
        "  #momentum_change_steps = 5000\n",
        "  step = 0  # Step count\n",
        "\n",
        "  losses = []\n",
        "  model1.train()\n",
        "  for epoch in range(50):\n",
        "      epoch_loss = []\n",
        "\n",
        "      # Iterate over data\n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):  \n",
        "        #move tensor to the same device (CPU/GPU) as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "      \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()  \n",
        "      \n",
        "        # forward + loss calc + backward + step\n",
        "        outputs = model1(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #test_lr_scheduler.step()\n",
        "\n",
        "      \n",
        "      \n",
        "        #momentum = final_momentum - (final_momentum - initial_momentum)*np.exp(-float(step)/momentum_change_steps)\n",
        "        #update_optim(optimizer, momentum, 'momentum')\n",
        "      \n",
        "\n",
        "        step += 1\n",
        "\n",
        "        #if batch_idx % 600 == 0:\n",
        "        #  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "      losses.append(np.mean(epoch_loss))\n",
        "\n",
        "  plt.plot(np.arange(len(losses)), losses, 'r')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          images = images.view(-1, 28*28)\n",
        "          outputs = model1(images)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.0 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.0 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.74 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.034999999999999996 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.014000000000000002 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.85 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.06999999999999999 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.028000000000000004 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.7 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.105 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.042 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.81 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.13999999999999999 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.05600000000000001 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.93 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.175 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.07 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.97 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.21 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.084 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.94 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.24499999999999997 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.098 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.93 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.27999999999999997 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.11200000000000002 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 98.03 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.315 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.12600000000000003 , prob_mask_prop=1 , method=standard)\n",
            ")\n",
            "Test Accuracy: 97.89 %\n",
            "Network Architecture: \n",
            " MLP1(\n",
            "  (hidden_layer1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (hidden_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (hidden_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (output_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (custom_dropout_HL): CustomDropout(p=0.35 , prob_mask_prop=1 , method=standard)\n",
            "  (custom_dropout_input): CustomDropout(p=0.14 , prob_mask_prop=1 , method=standard)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_CPKgJyFnst"
      },
      "source": [
        "# CIFAR10 Experiments (unused)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3A8k2DDGGK-"
      },
      "source": [
        "Dataloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAbaDAzbGdis"
      },
      "source": [
        "#CIFAR10 dataloading\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=len(trainset))\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = (images.numpy()).transpose(0,2,3,1)\n",
        "images = images.reshape(-1, 3072)\n",
        "images = images - images.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV1Xvj5ZKDQ1"
      },
      "source": [
        "#Preprocessing training set example\n",
        "zca_data = np.zeros(3072) #Temporarily add initial row of zeros so vstack works\n",
        "for i in range(5):\n",
        "  images_temp = images[i*10000:(i+1)*10000]\n",
        "  # compute the covariance of the image data\n",
        "  cov = np.cov(images_temp, rowvar=True)   # cov is (N, N)\n",
        "  # singular value decomposition\n",
        "  U,S,V = np.linalg.svd(cov)     # U is (N, N), S is (N,)\n",
        "  # build the ZCA matrix\n",
        "  epsilon = 1e-5\n",
        "  zca_matrix = np.dot(U, np.dot(np.diag(1.0/np.sqrt(S + epsilon)), U.T))\n",
        "  # transform the image data       zca_matrix is (N,N)\n",
        "  zca = np.dot(zca_matrix, images_temp)    # zca is (N, 3072)\n",
        "  for j in range(10000):\n",
        "    zca_data = np.vstack((zca_data,zca[j]))\n",
        "zca_data = zca_data[1:50002]\n",
        "zca_data_resized = zca_data.reshape(-1,3,32,32)\n",
        "tensor_x = torch.Tensor(zca_data_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZVQxQ_PGTuW"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jcZytw9GJzg"
      },
      "source": [
        "#CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 96, 5, padding=2) \n",
        "        self.pool = nn.MaxPool2d(3, 2)\n",
        "        self.conv2 = nn.Conv2d(96, 128, 5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 5, padding=2)\n",
        "        self.fc1 = nn.Linear(256*3*3, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.fc3 = nn.Linear(2048, 10)\n",
        "        if config['activation'] == 'ReLU':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif config['activation'] == 'leaky_ReLU':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.1)\n",
        "        self.dropout1 = nn.Dropout(p=config['dropout'][0])\n",
        "        self.dropout2 = nn.Dropout(p=config['dropout'][1])\n",
        "        self.dropout3 = nn.Dropout(p=config['dropout'][2])\n",
        "        self.dropout4 = nn.Dropout(p=config['dropout'][3])\n",
        "        self.dropout5 = nn.Dropout(p=config['dropout'][4])\n",
        "        self.dropout6 = nn.Dropout(p=config['dropout'][5])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(self.activation(self.conv1(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool(self.activation(self.conv2(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = self.pool(self.activation(self.conv3(x)))\n",
        "        x = self.dropout4(x)\n",
        "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.dropout5(x)\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.dropout6(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNOoT3-cGVR5"
      },
      "source": [
        "Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnFN_yCiFtv6"
      },
      "source": [
        "#Table 4: CNN without dropout\n",
        "config = {\n",
        "    'batch_size': (264,),\n",
        "    'early_stopping': (True,),\n",
        "    'max_epochs': (500,),\n",
        "    'max_steps': (None,),  # If None need to specify max_epochs\n",
        "    'momentum': (0.95,),\n",
        "    'momentum_decay': (True,), # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': (0.5,),\n",
        "    'final_momentum': (0.95,),\n",
        "    'momentum_change_steps': (20000,), # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': (0.25,0.1,0.01,0.001), # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': ('EXPONENTIAL',), # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1,),\n",
        "    'epsilon_decay_half_life': (100000,),\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': ([0,0,0,0,0,0],), # Implement your models with each layer's p in a list\n",
        "    'activation': ('ReLU',),\n",
        "    'D_in': (32*32,),\n",
        "    'D_h': ([0],), # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,), # Number of classes\n",
        "    'max_norm': (2,),  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'CNN' # MLP or CNN\n",
        "}\n",
        "train_test(trainset, testset, config, RAND, k_folds=0.1, random_search_samples=4, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHw_yXpXF3lY"
      },
      "source": [
        "#Table 4: CNN with dropout only in fully connected layers\n",
        "config = {\n",
        "    'batch_size': (264,),\n",
        "    'early_stopping': (True,),\n",
        "    'max_epochs': (500,),\n",
        "    'max_steps': (None,),  # If None need to specify max_epochs\n",
        "    'momentum': (0.95,),\n",
        "    'momentum_decay': (True,), # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': (0.5,),\n",
        "    'final_momentum': (0.95,),\n",
        "    'momentum_change_steps': (20000,), # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': (0.25,0.1,0.01,0.001), # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': ('EXPONENTIAL',), # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1,),\n",
        "    'epsilon_decay_half_life': (100000,),\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': ([0,0,0,0,0.5,0.5],), # Implement your models with each layer's p in a list\n",
        "    'activation': ('ReLU',),\n",
        "    'D_in': (32*32,),\n",
        "    'D_h': ([0],), # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,), # Number of classes\n",
        "    'max_norm': (2,),  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'CNN' # MLP or CNN\n",
        "}\n",
        "train_test(trainset, testset, config, RAND, k_folds=0.1, random_search_samples=4, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPYYjJ-fF3qc"
      },
      "source": [
        "#Table 4: CNN with dropout in all layers\n",
        "config = {\n",
        "    'batch_size': (264,),\n",
        "    'early_stopping': (True,),\n",
        "    'max_epochs': (500,),\n",
        "    'max_steps': (None,),  # If None need to specify max_epochs\n",
        "    'momentum': (0.95,),\n",
        "    'momentum_decay': (True,), # If True will use 'initial_momentum' and 'final_momentum' according to schedule, if False will use 'momentum'\n",
        "    'initial_momentum': (0.5,),\n",
        "    'final_momentum': (0.95,),\n",
        "    'momentum_change_steps': (20000,), # Number of steps at which 'final_momentum' is set\n",
        "    'learning_rate': (0.25,0.1,0.01,0.001), # (0.25, 1e-1, 1e-2),\n",
        "    'epsilon_decay': ('EXPONENTIAL',), # EXPONENTIAL OR INVERSE_T, for learning rate decay, False for static learning rate\n",
        "    'base_epsilon': (0.1,),\n",
        "    'epsilon_decay_half_life': (100000,),\n",
        "    'L2_regularization_param': (0,),\n",
        "    'L1_regularization_param': 0.0,\n",
        "    'L1_start_step': 150000,\n",
        "    'kl_sparsity': None,  # KL Sparsity rho\n",
        "    'kl_sparsity_beta': 0.0,  # beta coefficient for regularization term\n",
        "    'dropout': ([0.9,0.75,0.75,0.5,0.5,0.5],), # Implement your models with each layer's p in a list\n",
        "    'activation': ('ReLU',),\n",
        "    'D_in': (32*32,),\n",
        "    'D_h': ([0],), # Width of each hidden layer inside a list, if D_h[0] is 0 then no hidden layers\n",
        "    'D_out': (10,), # Number of classes\n",
        "    'max_norm': (2,),  # (4, 3, 2, 1, 0.5, None] # Max norm contraint from dropout paper on the weights\n",
        "    'model_type': 'CNN' # MLP or CNN\n",
        "}\n",
        "train_test(trainset, testset, config, RAND, k_folds=0.1, random_search_samples=4, verbose=True, plot_acc=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}